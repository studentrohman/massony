{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"spacy_trainer.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CGTpCDtEmDI-"},"source":["# Required Library"]},{"cell_type":"code","metadata":{"id":"YrIIlucWktil","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610122359211,"user_tz":-420,"elapsed":7134,"user":{"displayName":"Teknodata Inovasi","photoUrl":"","userId":"02067382072579067067"}},"outputId":"6d230f68-0298-40d6-e927-6d1b5c38e439"},"source":["!pip install spacy\n","!pip install jsonlines"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.19.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (51.1.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.3.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n","Collecting jsonlines\n","  Downloading https://files.pythonhosted.org/packages/d4/58/06f430ff7607a2929f80f07bfd820acbc508a4e977542fefcc522cde9dff/jsonlines-2.0.0-py3-none-any.whl\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-2.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5A6F-18BkVTl","executionInfo":{"status":"ok","timestamp":1610122359745,"user_tz":-420,"elapsed":7655,"user":{"displayName":"Teknodata Inovasi","photoUrl":"","userId":"02067382072579067067"}}},"source":["import os, sys\n","import re\n","from google.colab import drive\n","import spacy\n","import random\n","import re\n","from spacy.util import minibatch, compounding\n","from spacy.scorer import Scorer\n","from spacy.gold import GoldParse\n","from spacy import displacy\n","import pandas as pd\n","import jsonlines\n","import random\n","import math\n","from pathlib import Path"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"UsmlJF5-qcgY","colab":{"base_uri":"https://localhost:8080/","height":387},"executionInfo":{"status":"error","timestamp":1607741923022,"user_tz":-420,"elapsed":36462,"user":{"displayName":"Pesantren Kita","photoUrl":"","userId":"11609906095759275806"}},"outputId":"848a446b-e3d7-4e02-b12e-67d0976a365a"},"source":["drive.mount('/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly&response_type=code\n","\n","Enter your authorization code:\n","ssss\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-f9f4e770b961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed: invalid oauth code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrote_to_fifo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed: invalid oauth code"]}]},{"cell_type":"markdown","metadata":{"id":"aCUnqS6Xtal1"},"source":["# Named Entity Recognition\n"]},{"cell_type":"markdown","metadata":{"id":"l1iIhdHSl9EJ"},"source":["## NER dataset\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"SVO-wCm9mL4M"},"source":["'''\n","Dataset yang diperlukan memiliki format seperti berikut yang merupakan list of tuples.\n","[(\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n","(\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]})]\n","'''\n","dataset = jsonlines.open('/drive/My Drive/9. NLP 101 & NLP modelling using spaCy/dataset/data-training.json')\n","dataset = [(data['text'], {\"entities\": data['labels']}) for data in dataset if len(data['labels']) != 0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5l86Itvj-dK"},"source":["dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxm5DfHfn1DU"},"source":["split_length = math.ceil(len(dataset) * 0.85)\n","random.shuffle(dataset)\n","TRAIN_DATA = dataset[:split_length]\n","TEST_DATA = dataset[split_length:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HfUOzdWgsNJX"},"source":["##**NER Trainer** "]},{"cell_type":"code","metadata":{"id":"8KyamxKAsPLo"},"source":[" path_MODEL = os.path.abspath('/drive/My Drive/9. NLP 101 & NLP modelling using spaCy/model/id_maslahah_ner')\n","blank = True\n","n_iter = 100\n","\n","if blank:\n","    nlp = spacy.blank(\"id\")  # create blank Language class\n","    print(\"Created blank 'id' ner model\")\n","else:\n","    nlp = spacy.load(path_MODEL)  # load existing spaCy model\n","    print(\"Loaded model '%s'\" % model)\n","\n","# create the built-in pipeline components and add them to the pipeline\n","# nlp.create_pipe works for built-ins that are registered with spaCy\n","if \"ner\" not in nlp.pipe_names:\n","    ner = nlp.create_pipe(\"ner\")\n","    nlp.add_pipe(ner, last=True)\n","\n","# otherwise, get it so we can add labels\n","else:\n","    ner = nlp.get_pipe(\"ner\")\n","\n","# add labels\n","for _, annotations in TRAIN_DATA:\n","    for ent in annotations.get(\"entities\"):\n","        ner.add_label(ent[2])\n","\n","# get names of other pipes to disable them during training\n","pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n","other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n","with nlp.disable_pipes(*other_pipes):  # only train NER\n","  optimizer = nlp.begin_training()\n","  for itn in range(n_iter):\n","      random.shuffle(TRAIN_DATA)\n","      losses = {}\n","      # batch up the examples using spaCy's minibatch\n","      batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n","      for batch in batches:\n","          texts, annotations = zip(*batch)\n","          nlp.update(\n","              texts,  # batch of texts\n","              annotations,  # batch of annotations\n","              drop=0.4,  # dropout - make it harder to memorise data\n","              sgd=optimizer,\n","              losses=losses)\n","      print(\"Losses\", losses)\n","\n","scorer_test = Scorer()\n","for text, _ann_ in TEST_DATA:\n","    doc = nlp(text)\n","    doc_gold_text = nlp.make_doc(text)\n","    gold = GoldParse(doc_gold_text, entities=_ann_['entities'])\n","    try:\n","        scorer_test.score(doc, gold)\n","    except:\n","        continue\n","\n","output_dir = Path(path_MODEL)\n","if not output_dir.exists():\n","  output_dir.mkdir()\n","nlp.to_disk(output_dir)\n","print(\"Saved model to\", output_dir)\n","print(\"{}\\n\\n\".format(scorer_test.scores))\n","\n","# test model\n","model = spacy.load(path_MODEL)\n","text = \"Joko Widodo adalah Presiden Indonesia\"\n","doc = model(text)\n","for ent in doc.ents:\n","  print (f'{ent.text} is {ent.label_}')\n","\n","# test model menggunakan displacy\n","# displacy.serve(doc, style=\"ent\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4QeewcZ03RG5"},"source":["model = spacy.load(path_MODEL)\n","text = \"sya membayar 45 rb\"\n","doc = model(text)\n","for ent in doc.ents:\n","  print (f'{ent.text} is {ent.label_}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYWcsx5wtrUG"},"source":["# Sentiment Analysis\n","Di dalam spacy sendiri, kita bisa menggunakan custom model ketika melakukan training model yang berbentuk dokumen klasifikasi. Pada kali ini contoh yang diambil adalah menggunakan model dengan arsitektur nya adalah simple-cnn dan menggunakan \n","TextCategorizer component. Data yang dipakai saya dapatkan dari Github, berupa dataset review produk di website Female Daily.\n"]},{"cell_type":"markdown","metadata":{"id":"oh0Ad9XelRuu"},"source":["## Sentiment Dataset"]},{"cell_type":"code","metadata":{"id":"KPMyxvYEBh-L","executionInfo":{"status":"ok","timestamp":1610122359748,"user_tz":-420,"elapsed":2140,"user":{"displayName":"Teknodata Inovasi","photoUrl":"","userId":"02067382072579067067"}}},"source":["import pandas as pd"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQg2o799dYeG","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1610122361128,"user_tz":-420,"elapsed":1291,"user":{"displayName":"Teknodata Inovasi","photoUrl":"","userId":"02067382072579067067"}},"outputId":"0c72de78-57a1-4e85-95ff-e42fa58055bd"},"source":["url = 'https://raw.githubusercontent.com/chlaudiah/Sentiment-Classification-FD-Reviews/master/dataset.csv'\n","df = pd.read_csv(url, error_bad_lines=False)\n","df"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID Data</th>\n","      <th>Nama Produk</th>\n","      <th>Jenis Kulit</th>\n","      <th>Review Produk</th>\n","      <th>Rating</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A001</td>\n","      <td>Cetaphil: Gentle Skin Cleanser</td>\n","      <td>Oily</td>\n","      <td>Aku suka banget sama ini. Cuma ini cleanser ya...</td>\n","      <td>5</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A002</td>\n","      <td>Cetaphil: Gentle Skin Cleanser</td>\n","      <td>Oily</td>\n","      <td>Walaupun produknya mengklaim dapat digunakan s...</td>\n","      <td>5</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A003</td>\n","      <td>Cetaphil: Gentle Skin Cleanser</td>\n","      <td>Dry</td>\n","      <td>Thanks God for this product. Produk ini udah p...</td>\n","      <td>5</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A004</td>\n","      <td>Cetaphil: Gentle Skin Cleanser</td>\n","      <td>Oily</td>\n","      <td>Another favorite product from Cetaphil. Seneng...</td>\n","      <td>5</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>A005</td>\n","      <td>Cetaphil: Gentle Skin Cleanser</td>\n","      <td>Oily</td>\n","      <td>awalnya kaget karena cleanser ini enggak berbu...</td>\n","      <td>5</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>A496</td>\n","      <td>Acnes: Creamy Wash</td>\n","      <td>Dry</td>\n","      <td>Aku beli ini karna baca blog female daily 3 cl...</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>A497</td>\n","      <td>Acnes: Creamy Wash</td>\n","      <td>Dry</td>\n","      <td>aku pake ini udh ngabisin 1 botol, saat aku je...</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>A498</td>\n","      <td>Acnes: Creamy Wash</td>\n","      <td>Dry</td>\n","      <td>Facial Wash yg aku beli pas awal2 mulai jerawa...</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>A499</td>\n","      <td>Acnes: Creamy Wash</td>\n","      <td>Dry</td>\n","      <td>Aku pernah beli ini dengan harapan bisa bantu ...</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>A500</td>\n","      <td>Acnes: Creamy Wash</td>\n","      <td>Dry</td>\n","      <td>Beli ini waktu awal kuliah pas banyak jerawat....</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 6 columns</p>\n","</div>"],"text/plain":["    ID Data                     Nama Produk  ... Rating     Label\n","0      A001  Cetaphil: Gentle Skin Cleanser  ...      5  Positive\n","1      A002  Cetaphil: Gentle Skin Cleanser  ...      5  Positive\n","2      A003  Cetaphil: Gentle Skin Cleanser  ...      5  Positive\n","3      A004  Cetaphil: Gentle Skin Cleanser  ...      5  Positive\n","4      A005  Cetaphil: Gentle Skin Cleanser  ...      5  Positive\n","..      ...                             ...  ...    ...       ...\n","495    A496              Acnes: Creamy Wash  ...      1  negative\n","496    A497              Acnes: Creamy Wash  ...      1  negative\n","497    A498              Acnes: Creamy Wash  ...      1  negative\n","498    A499              Acnes: Creamy Wash  ...      1  negative\n","499    A500              Acnes: Creamy Wash  ...      1  negative\n","\n","[500 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"eDLJBZg7hruJ","executionInfo":{"status":"ok","timestamp":1610122368096,"user_tz":-420,"elapsed":1152,"user":{"displayName":"Teknodata Inovasi","photoUrl":"","userId":"02067382072579067067"}}},"source":["def load_data_sentiment(ratio=0.85):\n","  split = math.ceil(len(df['Review Produk']) * 0.85)\n","  texts = list(df['Review Produk'])\n","  cats = [{\"POSITIVE\": True, \"NEGATIVE\": False} if label.lower() == \"positive\" else {\"POSITIVE\": False, \"NEGATIVE\": True} for label in list(df['Label'])]\n","  return (texts[:split], cats[:split]), (texts[split:], cats[split:])"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"k78RK0CQq03o","executionInfo":{"status":"ok","timestamp":1610122370112,"user_tz":-420,"elapsed":1051,"user":{"displayName":"Teknodata Inovasi","photoUrl":"","userId":"02067382072579067067"}}},"source":["(train_texts, train_cats), (dev_texts, dev_cats) = load_data_sentiment(ratio=0.85)\n","train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z_ZE2Qm_COoS"},"source":["train_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QazVHrRiox_y"},"source":["## Evaluation Function"]},{"cell_type":"code","metadata":{"id":"EoH-MPQYo1ba","executionInfo":{"status":"ok","timestamp":1610122374094,"user_tz":-420,"elapsed":1391,"user":{"displayName":"Teknodata Inovasi","photoUrl":"","userId":"02067382072579067067"}}},"source":["def evaluate(tokenizer, textcat, texts, cats):\n","  docs = (tokenizer(text) for text in texts)\n","  tp = 0.0  # True positives\n","  fp = 1e-8  # False positives\n","  fn = 1e-8  # False negatives\n","  tn = 0.0  # True negatives\n","  for i, doc in enumerate(textcat.pipe(docs)):\n","      gold = cats[i]\n","      for label, score in doc.cats.items():\n","          if label not in gold:\n","              continue\n","          if label == \"NEGATIVE\":\n","              continue\n","          if score >= 0.5 and gold[label] >= 0.5:\n","              tp += 1.0\n","          elif score >= 0.5 and gold[label] < 0.5:\n","              fp += 1.0\n","          elif score < 0.5 and gold[label] < 0.5:\n","              tn += 1\n","          elif score < 0.5 and gold[label] >= 0.5:\n","              fn += 1\n","  precision = tp / (tp + fp)\n","  recall = tp / (tp + fn)\n","  if (precision + recall) == 0:\n","      f_score = 0.0\n","  else:\n","      f_score = 2 * (precision * recall) / (precision + recall)\n","  return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aHs1KjG2lXez"},"source":["## Sentiment Trainer"]},{"cell_type":"code","metadata":{"id":"145goRi9lbKy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610122759531,"user_tz":-420,"elapsed":366074,"user":{"displayName":"Teknodata Inovasi","photoUrl":"","userId":"02067382072579067067"}},"outputId":"ea4e9ab8-b1e0-4d39-ec14-19ce1bff94b0"},"source":["path_MODEL = os.path.abspath('/content/sample_data')\n","blank = True\n","n_iter = 75\n","\n","if blank:\n","    nlp = spacy.blank(\"id\")  # create blank Language class\n","    print(\"Created blank 'id' sentiment model\")\n","else:\n","    nlp = spacy.load(path_MODEL)  # load existing spaCy model\n","    print(\"Loaded model '%s'\" % model)\n","\n","# create the built-in pipeline components and add them to the pipeline\n","# nlp.create_pipe works for built-ins that are registered with spaCy\n","if \"textcat\" not in nlp.pipe_names:\n","    textcat = nlp.create_pipe(\n","        \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"}\n","    )\n","    nlp.add_pipe(textcat, last=True)\n","# otherwise, get it, so we can add labels to it\n","else:\n","    textcat = nlp.get_pipe(\"textcat\")\n","\n","# add label to text classifier\n","textcat.add_label(\"POSITIVE\")\n","textcat.add_label(\"NEGATIVE\")\n","\n","(train_texts, train_cats), (dev_texts, dev_cats) = load_data_sentiment(ratio=0.85)\n","\n","print(\"Using examples ({} training, {} evaluation)\".format(len(train_texts), len(dev_texts)))\n","train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n","\n","# get names of other pipes to disable them during training\n","pipe_exceptions = [\"textcat\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n","other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n","with nlp.disable_pipes(*other_pipes):  # only train textcat\n","    optimizer = nlp.begin_training()\n","    print(\"Training the model...\")\n","    print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n","    batch_sizes = compounding(4.0, 32.0, 1.001)\n","    for i in range(n_iter):\n","        losses = {}\n","        # batch up the examples using spaCy's minibatch\n","        random.shuffle(train_data)\n","        batches = minibatch(train_data, size=batch_sizes)\n","        for batch in batches:\n","            texts, annotations = zip(*batch)\n","            nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n","        with textcat.model.use_params(optimizer.averages):\n","            # evaluate on the dev data split off in load_data()\n","            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n","        print(\n","            \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(  # print a simple table\n","                losses[\"textcat\"],\n","                scores[\"textcat_p\"],\n","                scores[\"textcat_r\"],\n","                scores[\"textcat_f\"],\n","            )\n","        )\n","\n","output_dir = Path(path_MODEL)\n","if not output_dir.exists():\n","  output_dir.mkdir()\n","nlp.to_disk(output_dir)\n","print(\"Saved model to\", output_dir)\n","\n","\n","# test the trained model\n","test_text = \"Aku suka banget sama ini. Cuma ini cleanser yang cocok buat aku. Aku udah sering banget gonta-ganti cleanser karena wajahku yang penuh jerawat. Aku udah coba yang low-end dan high-end sekalipun tapi tetep gak ada yang sebagus ini. Karena produknya gentle jadi gak bikin iritasi diwajah, kebanyakan produk cleanser lain yang harsh untuk wajah justru dapat menyebabkan iritasi sehingga munculah jerawat. Very recommended!\"\n","doc = nlp(test_text)\n","print(test_text, doc.cats)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Created blank 'id' sentiment model\n","Using examples (425 training, 75 evaluation)\n","Training the model...\n","LOSS \t  P  \t  R  \t  F  \n","2.954\t0.794\t0.794\t0.794\n","1.335\t0.829\t0.853\t0.841\n","0.236\t0.818\t0.794\t0.806\n","0.055\t0.862\t0.735\t0.794\n","0.018\t0.867\t0.765\t0.812\n","0.002\t0.875\t0.824\t0.848\n","0.000\t0.875\t0.824\t0.848\n","0.000\t0.882\t0.882\t0.882\n","0.000\t0.875\t0.824\t0.848\n","0.000\t0.882\t0.882\t0.882\n","0.000\t0.882\t0.882\t0.882\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.879\t0.853\t0.866\n","0.000\t0.879\t0.853\t0.866\n","0.000\t0.879\t0.853\t0.866\n","0.000\t0.879\t0.853\t0.866\n","0.000\t0.879\t0.853\t0.866\n","0.000\t0.882\t0.882\t0.882\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","0.000\t0.857\t0.882\t0.870\n","Saved model to /content/sample_data\n","Aku suka banget sama ini. Cuma ini cleanser yang cocok buat aku. Aku udah sering banget gonta-ganti cleanser karena wajahku yang penuh jerawat. Aku udah coba yang low-end dan high-end sekalipun tapi tetep gak ada yang sebagus ini. Karena produknya gentle jadi gak bikin iritasi diwajah, kebanyakan produk cleanser lain yang harsh untuk wajah justru dapat menyebabkan iritasi sehingga munculah jerawat. Very recommended! {'POSITIVE': 0.9999945163726807, 'NEGATIVE': 5.525071628653677e-06}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"stdKmJXTo9-W"},"source":["nlp = spacy.load(os.path.abspath('/drive/My Drive/9. NLP 101 & NLP modelling using spaCy/model/id_maslahah_sentiment'))\n","text = list(df['Review Produk']) if str(incom) != 'nan'\n","predict = [nlp(data).cats.items() for data in text]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ax13KHZzCc60"},"source":["predict_result = pd.DataFrame(\n","    {'text': text,\n","     'prediction': predict\n","    })"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2JY6UkyiC17Y"},"source":["predict_result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6x4Pc1JEC3kr"},"source":[""],"execution_count":null,"outputs":[]}]}